---
title: 'Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic
  analysis'
abstract: Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of Stochastic
  Gradient Descent, where properly scaled isotropic Gaussian noise is added to an
  unbiased estimate of the gradient at each iteration. This modest change allows SGLD
  to escape local minima and suffices to guarantee asymptotic convergence to global
  minimizers for sufficiently regular non-convex objectives. The present work provides
  a nonasymptotic analysis in the context of non-convex learning problems, giving
  finite-time guarantees for SGLD to find approximate minimizers of both empirical
  and population risks. As in the asymptotic setting, our analysis relates the discrete-time
  SGLD Markov chain to a continuous-time diffusion process. A new tool that drives
  the results is the use of weighted transportation cost inequalities to quantify
  the rate of convergence of SGLD to a stationary distribution in the Euclidean $2$-Wasserstein
  distance.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: raginsky17a
month: 0
tex_title: 'Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic
  analysis'
firstpage: 1674
lastpage: 1703
page: 1674-1703
order: 1674
cycles: false
author:
- given: Maxim
  family: Raginsky
- given: Alexander
  family: Rakhlin
- given: Matus
  family: Telgarsky
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/raginsky17a/raginsky17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
