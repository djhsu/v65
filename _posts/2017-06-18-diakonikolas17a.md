---
title: Learning Multivariate Log-concave Distributions
abstract: 'We study the problem of estimating multivariate log-concave probability
  density functions. We prove the first sample complexity upper bound for learning
  log-concave densities on $\mathbbR^d$, for all $d ≥1$. Prior to our work, no upper
  bound on the sample complexity of this learning problem was known for the case of
  $d>3$. In more detail, we give an estimator that, for any $d \ge 1$ and $ε>0$, draws
  $\tildeO_d \left( (1/ε)^(d+5)/2 \right)$ samples from an unknown target log-concave
  density on $\R^d$, and outputs a hypothesis that (with high probability) is $ε$-close
  to the target, in total variation distance. Our upper bound on the sample complexity
  comes close to the known lower bound of $\Omega_d \left( (1/ε)^(d+1)/2 \right)$
  for this problem. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: diakonikolas17a
month: 0
tex_title: Learning Multivariate Log-concave Distributions
firstpage: 711
lastpage: 727
page: 711-727
order: 711
cycles: false
author:
- given: Ilias
  family: Diakonikolas
- given: Daniel M.
  family: Kane
- given: Alistair
  family: Stewart
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/diakonikolas17a/diakonikolas17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
