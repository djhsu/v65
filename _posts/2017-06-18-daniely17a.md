---
title: Depth Separation for Neural Networks
abstract: Let $f:\mathbb{S}^d-1\times \mathbb{S}^d-1\to\mathbb{S}$ be a function of the
  form $f(\x,\x’) = g(⟨\x,\x’⟩)$ for $g:[-1,1]\to \mathbbR$. We give a simple proof
  that shows that poly-size depth two neural networks with (exponentially) bounded
  weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree
  polynomial. Moreover, for many $g$’s, such as $g(x)=\sin(\pi d^3x)$, the number
  of neurons must be $2^Ω\left(d\log(d)\right)$. Furthermore, the result holds w.r.t.
  the uniform distribution on $\mathbb{S}^d-1\times \mathbb{S}^d-1$. As many functions
  of the above form can be well approximated by poly-size depth three networks with
  poly-bounded weights, this establishes a separation between depth two and depth
  three networks w.r.t. the uniform distribution on $\mathbb{S}^d-1\times \mathbb{S}^d-1$.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: daniely17a
month: 0
tex_title: Depth Separation for Neural Networks
firstpage: 690
lastpage: 696
page: 690-696
order: 690
cycles: false
author:
- given: Amit
  family: Daniely
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/daniely17a/daniely17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
