---
title: Sparse Stochastic Bandits
abstract: In the classical multi-armed bandit problem, $d$ arms are available to the
  decision maker who pulls them sequentially in order to maximize his cumulative reward.
  Guarantees can be obtained on a relative quantity called regret, which scales linearly
  with $d$ (or with $\sqrt{d}$ in the minimax sense). We here consider the \emphsparse
  case of this classical problem in the sense that only a small number of arms, namely
  $s<d$, have a \emphpositive expected reward. We are able to leverage this additional
  assumption to provide an algorithm whose regret scales with $s$ instead of $d$.
  Moreover, we prove that this algorithm is optimal by providing a matching lower
  bound – at least for a wide and pertinent range of parameters that we determine
  – and by evaluating its performance on simulated data.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kwon17a
month: 0
tex_title: Sparse Stochastic Bandits
firstpage: 1269
lastpage: 1270
page: 1269-1270
order: 1269
cycles: false
author:
- given: Joon
  family: Kwon
- given: Vianney
  family: Perchet
- given: Claire
  family: Vernade
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/kwon17a/kwon17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
