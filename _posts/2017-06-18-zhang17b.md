---
title: A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics
abstract: 'We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for
  non-convex optimization. The algorithm performs stochastic gradient descent, where
  in each step it injects appropriately scaled Gaussian noise to the update. We analyze
  the algorithmâ€™s hitting time to an arbitrary subset of the parameter space. Two
  results follow from our general theory: First, we prove that for empirical risk
  minimization, if the empirical risk is point-wise close to the (smooth) population
  risk, then the algorithm achieves an approximate local minimum of the population
  risk in polynomial time, escaping suboptimal local minima that only exist in the
  empirical risk. Second, we show that SGLD improves on one of the best known learnability
  results for learning linear classifiers under the zero-one loss.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang17b
month: 0
tex_title: A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics
firstpage: 1980
lastpage: 2022
page: 1980-2022
order: 1980
cycles: false
author:
- given: Yuchen
  family: Zhang
- given: Percy
  family: Liang
- given: Moses
  family: Charikar
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/zhang17b/zhang17b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
