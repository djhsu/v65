---
title: Nearly-tight VC-dimension bounds for piecewise linear neural networks
abstract: We prove new upper and lower bounds on the VC-dimension of deep neural networks
  with the ReLU activation function. These bounds are tight for almost the entire
  range of parameters. Letting $W$ be the number of weights and $L$ be the number
  of layers, we prove that the VC-dimension is $O(W L \log(W))$, and provide examples
  with VC-dimension $Ω( W L \log(W/L) )$. This improves both the previously known
  upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we
  prove a tight bound $Θ(W U)$ on the VC-dimension. All of these results generalize
  to arbitrary piecewise linear activation functions.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: harvey17a
month: 0
tex_title: Nearly-tight {VC}-dimension bounds for piecewise linear neural networks
firstpage: 1064
lastpage: 1068
page: 1064-1068
order: 1064
cycles: false
author:
- given: Nick
  family: Harvey
- given: Christopher
  family: Liaw
- given: Abbas
  family: Mehrabian
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/harvey17a/harvey17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
