---
title: Surprising properties of dropout in deep networks
abstract: We analyze dropout in deep networks with rectified linear units and the
  quadratic loss. Our results expose surprising differences between the behavior of
  dropout and more traditional regularizers like weight decay. For example, on some
  simple data sets dropout training produces negative weights even though the output
  is the sum of the inputs. This provides a counterpoint to the suggestion that dropout
  discourages co-adaptation of weights. We also  show that the dropout penalty can
  grow exponentially in the depth of the network while the weight-decay penalty remains
  essentially linear, and that dropout is insensitive to various re-scalings of the
  input features, outputs, and network weights. This last insensitivity implies that
  there are no isolated local minima of the dropout training criterion. Our work uncovers
  new properties of dropout, extends our understanding of why dropout succeeds, and
  lays the foundation for further progress.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: helmbold17a
month: 0
tex_title: Surprising properties of dropout in deep networks
firstpage: 1123
lastpage: 1146
page: 1123-1146
order: 1123
cycles: false
author:
- given: David P.
  family: Helmbold
- given: Philip M.
  family: Long
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/helmbold17a/helmbold17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
