---
title: The Sample Complexity of Optimizing a Convex Function
abstract: In this paper we study optimization from samples of convex functions. There
  are many scenarios in which we do not know the function we wish to optimize but
  can learn it from data.  In such cases,  we are interested in bounding the number
  of samples required to optimize the function.   Our main result shows that in general,  the
  number of samples required to obtain a non-trivial approximation to the optimum
  of a convex function is exponential in its dimension, even when the function is
  PAC-learnable. We also obtain strong lower bounds for strongly convex and Lipschitz
  continuous functions. On the positive side, we show that there are interesting classes
  of functions and distributions for which the sample complexity is polynomial in
  the dimension of the function.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: balkanski17a
month: 0
tex_title: The Sample Complexity of Optimizing a Convex Function
firstpage: 275
lastpage: 301
page: 275-301
order: 275
cycles: false
author:
- given: Eric
  family: Balkanski
- given: Yaron
  family: Singer
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/balkanski17a/balkanski17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
