---
title: Online Learning Without Prior Information
abstract: The vast majority of optimization and online learning algorithms today require
  some prior information about the data (often in the form of bounds on gradients
  or on the optimal parameter value). When this information is not available, these
  algorithms require laborious manual tuning of various hyperparameters, motivating
  the search for algorithms that can adapt to the data with no prior information.
  We  describe a frontier of new lower bounds on the performance of such algorithms,
  reflecting a tradeoff between a term that depends on the optimal parameter value
  and a term that depends on the gradientsâ€™ rate of growth. Further, we construct
  a family of algorithms whose performance matches any desired point on this frontier,
  which no previous algorithm reaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cutkosky17a
month: 0
tex_title: Online Learning Without Prior Information
firstpage: 643
lastpage: 677
page: 643-677
order: 643
cycles: false
author:
- given: Ashok
  family: Cutkosky
- given: Kwabena
  family: Boahen
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/cutkosky17a/cutkosky17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
