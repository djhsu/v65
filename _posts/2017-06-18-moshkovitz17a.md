---
title: Mixing Implies Lower Bounds for Space Bounded Learning
abstract: One can learn any hypothesis class H with O(log |H|) labeled examples. Alas,
  learning with so few examples requires saving the examples in memory, and this requires
  |X|^(O(log|H|)) memory states, where X is the set of all labeled examples. This
  motivates the question of how many labeled examples are needed in case the memory
  is bounded. Previous work showed, using techniques such as linear algebra and Fourier
  analysis, that parities cannot be learned with bounded memory and less than |H|^(Omega(1))
  examples. One might wonder whether a general combinatorial condition exists for
  unlearnability with bounded memory, as we have with the condition  VCdim(H) = Infinity
  for PAC unlearnability. In this paper we give such a condition. We show that if
  an hypothesis class H, when viewed as a bipartite graph between hypotheses H and
  labeled examples X, is mixing, then learning it requires |H|^(Omega(1)) examples
  under a certain bound on the memory. Note that the class of parities is mixing.
  Moreover, as an immediate corollary, we get that most hypothesis classes are unlearnable
  with bounded memory. Our proof technique is combinatorial in nature and very different
  from previous analyses.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: moshkovitz17a
month: 0
tex_title: Mixing Implies Lower Bounds for Space Bounded Learning
firstpage: 1516
lastpage: 1566
page: 1516-1566
order: 1516
cycles: false
author:
- given: Dana
  family: Moshkovitz
- given: Michal
  family: Moshkovitz
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/moshkovitz17a/moshkovitz17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
