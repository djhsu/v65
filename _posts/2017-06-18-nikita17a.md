---
abstract: Under margin assumptions, we prove several risk bounds, represented via
  the distribution dependent local entropies of the classes or the sizes of specific
  sample compression schemes. In some cases, our guarantees are optimal up to constant
  factors for families of classes. We discuss limitations of our approach and give
  several applications. In particular, we provide a new tight PAC bound for the hard-margin
  SVM, an extended analysis of certain empirical risk minimizers under log-concave
  distributions, a new variant of an online to batch conversion, and distribution
  dependent localized bounds in the aggregation framework. As a part of our results,
  we give a new upper bound for the uniform deviations under Bernstein assumptions,
  which may be of independent interest. The proofs for the sample compression schemes
  are based on the moment method combined with the analysis of voting algorithms.
title: Optimal learning via local entropies and sample compression
layout: inproceedings
series: Proceedings of Machine Learning Research
id: nikita17a
month: 0
tex_title: Optimal learning via local entropies and sample compression
firstpage: 2023
lastpage: 2065
page: 2023-2065
order: 2023
cycles: false
author:
- given: Zhivotovskiy
  family: Nikita
date: 2017-06-18
address: 
publisher: PMLR
container-title: Proceedings of the 2017 Conference on Learning Theory
volume: '65'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 6
  - 18
pdf: http://proceedings.mlr.press/v65/nikita17a/nikita17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
